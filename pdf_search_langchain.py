import os
import json
import tempfile
from typing import List, Dict, Any
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document


class PDFVectorSearch:
    def __init__(self):
        # Initialize with high-quality embeddings
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
            add_start_index=True,
            separators=["\n\n", "\n", " ", ""]
        )
        self.temp_dir = tempfile.mkdtemp(prefix="pdf_vector_")


    def pdf_to_vectorstore(self, pdf_path: str) -> str:
        """Convert PDF to vector store with enhanced processing"""
        try:
            # Load PDF with enhanced parsing
            loader = PyPDFLoader(pdf_path)
            documents = loader.load_and_split(self.text_splitter)

            # Add enhanced metadata
            for doc in documents:
                doc.metadata["content_hash"] = hash(doc.page_content)
                doc.metadata["is_legal_clause"] = self._is_legal_clause(doc.page_content)
                doc.metadata["is_table"] = self._is_table(doc.page_content)

            # Create FAISS index with larger dimensionality
            vectorstore = FAISS.from_documents(documents, self.embeddings)

            # Save with original content preservation
            base_name = os.path.splitext(os.path.basename(pdf_path))[0]
            vectorstore_path = os.path.join(self.temp_dir, f"{base_name}_faiss_index")
            vectorstore.save_local(vectorstore_path)

            # Save original content with context
            self._save_original_content(documents, base_name)

            return vectorstore_path

        except Exception as e:
            raise Exception(f"PDF processing failed: {str(e)}")


    def _is_legal_clause(self, text: str) -> bool:
        """Enhanced legal clause detection"""
        patterns = [
            "article", "section", "clause", "subsection",
            "¬ß", "¬∂", "‚Ä¢", "‚û¢", "‚ñ†", "‚ñ∫"
        ]
        return any(p in text.lower()[:100] for p in patterns)

    def _is_table(self, text: str) -> bool:
        """Enhanced table detection"""
        table_indicators = [
            "‚îå", "‚îê", "‚îî", "‚îò", "‚îú", "‚î§", "‚î¨", "‚î¥",
            "‚ïî", "‚ïó", "‚ïö", "‚ïù", "‚ïë", "‚ïê", "‚ï†", "‚ï£",
            "‚ï¶", "‚ï©", "‚ï¨", "‚îÇ", "‚îÄ", "‚îº"
        ]
        return any(indicator in text for indicator in table_indicators)

    def _save_original_content(self, documents: List[Document], base_name: str):
        """Save original content with context markers"""
        content_data = {
            "chunks": [
                {
                    "content_hash": doc.metadata["content_hash"],
                    "page_content": doc.page_content,
                    "metadata": doc.metadata,
                    "context_before": self._get_context(documents, idx, -1),
                    "context_after": self._get_context(documents, idx, 1)
                }
                for idx, doc in enumerate(documents)
            ]
        }
        with open(os.path.join(self.temp_dir, f"{base_name}_content.json"), 'w') as f:
            json.dump(content_data, f, indent=2)

    def _get_context(self, documents: List[Document], current_idx: int, offset: int) -> str:
        """Get surrounding context for better understanding"""
        target_idx = current_idx + offset
        if 0 <= target_idx < len(documents):
            return documents[target_idx].page_content[:300] + "..."
        return ""

    def semantic_search(self, vectorstore_path: str, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Perform deep semantic search with context"""
        try:
            # Load vector store
            vectorstore = FAISS.load_local(vectorstore_path, self.embeddings)

            # Enhanced similarity search with score threshold
            docs_and_scores = vectorstore.similarity_search_with_score(query, k=k*2)
            docs_and_scores = [d for d in docs_and_scores if d[1] > 0.5]  # Threshold

            # Load original content
            base_name = os.path.basename(vectorstore_path).replace("_faiss_index", "")
            content_path = os.path.join(self.temp_dir, f"{base_name}_content.json")

            with open(content_path, 'r') as f:
                content_data = json.load(f)

            # Prepare enriched results
            results = []
            for doc, score in docs_and_scores[:k]:
                original = next(
                    (c for c in content_data["chunks"]
                     if c["content_hash"] == doc.metadata["content_hash"]),
                    None
                )

                if original:
                    results.append({
                        "match_score": float(score),
                        "page_number": doc.metadata.get('page', 'N/A'),
                        "original_text": original["page_content"],
                        "context_before": original.get("context_before", ""),
                        "context_after": original.get("context_after", ""),
                        "metadata": {
                            "is_legal_clause": doc.metadata.get("is_legal_clause", False),
                            "is_table": doc.metadata.get("is_table", False),
                            "source": doc.metadata.get("source", "")
                        }
                    })

            return results

        except Exception as e:
            raise Exception(f"Search failed: {str(e)}")

    def cleanup(self):
        """Clean up temporary resources"""
        try:
            for root, dirs, files in os.walk(self.temp_dir, topdown=False):
                for name in files:
                    os.remove(os.path.join(root, name))
                for name in dirs:
                    os.rmdir(os.path.join(root, name))
            os.rmdir(self.temp_dir)
        except Exception as e:
            print(f"Cleanup warning: {str(e)}")

def main():
    import argparse

    parser = argparse.ArgumentParser(description="Deep PDF Semantic Search")
    parser.add_argument("pdf_path", help="Path to PDF document")
    parser.add_argument("query", help="Search description")
    parser.add_argument("--results", type=int, default=3, help="Number of results to return")
    args = parser.parse_args()

    searcher = PDFVectorSearch()

    try:
        # Process PDF
        print(f"üîç Processing PDF: {args.pdf_path}")
        vs_path = searcher.pdf_to_vectorstore(args.pdf_path)

        # Perform search
        print(f"‚ö° Searching for: '{args.query}'")
        results = searcher.semantic_search(vs_path, args.query, args.results)

        # Prepare output
        output = {
            "document": os.path.basename(args.pdf_path),
            "search_query": args.query,
            "search_config": {
                "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2",
                "max_results": args.results,
                "score_threshold": 0.5
            },
            "matches": results
        }

        # Print JSON output
        print(json.dumps(output, indent=2, ensure_ascii=False))

    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
    finally:
        searcher.cleanup()

if __name__ == "__main__":
    main()

## Key Features

# 1. **Enhanced PDF Processing**:
#    - Uses advanced multilingual BERT model for better semantic understanding
#    - Improved text splitting with context-aware chunking
#    - Detects legal clauses and tables with specialized heuristics
# 
# 2. **Deep Semantic Search**:
#    - Implements score thresholding to filter weak matches
#    - Returns contextual paragraphs before/after matches
#    - Preserves original text formatting and content
# 
# 3. **Complete JSON Output**:
# ```json
# {
#   "document": "contract.pdf",
#   "search_query": "termination clauses",
#   "search_config": {
#     "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2",
#     "max_results": 3,
#     "score_threshold": 0.5
#   },
#   "matches": [
#     {
#       "match_score": 0.891,
#       "page_number": 12,
#       "original_text": "Section 8.2 - Termination for Cause\n\nEither party may terminate...",
#       "context_before": "8.1 - General Provisions...",
#       "context_after": "8.3 - Notice Requirements...",
#       "metadata": {
#         "is_legal_clause": true,
#         "is_table": false,
#         "source": "contract.pdf"
#       }
#     }
#   ]
# }
# ```
# 
# 4. **Temporary Storage Management**:
#    - Creates dedicated temp directory
#    - Stores both FAISS index and original content
#    - Automatic cleanup after execution
# 
# ## Usage
# 
# ```bash
# python pdf_deep_search.py document.pdf "termination clauses" --results 5
# ```
# 
# ## Requirements
# 
# ```bash
# pip install langchain pypdf faiss-cpu sentence-transformers
# pip install -U langchain-community
# ```
# 
# For GPU support:
# ```bash
# pip install faiss-gpu
# ```
# 
# This solution provides:
# - High-quality semantic search using state-of-the-art embeddings
# - Complete original text preservation
# - Context-aware results with surrounding paragraphs
# - Professional JSON output format
# - Clean resource management
